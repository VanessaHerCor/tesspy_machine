"""
=============================================================================
T√âCNICA 4: BERT - EMBEDDINGS CONTEXTUALES (¬°LA REVOLUCI√ìN!)
=============================================================================

¬øQU√â ES?
--------
BERT (Bidirectional Encoder Representations from Transformers) es la revoluci√≥n
en Procesamiento del Lenguaje Natural.

A diferencia de Word2Vec:
  Word2Vec: Una palabra = UN SOLO vector (siempre igual)
  BERT: Una palabra = VECTORES DIFERENTES seg√∫n el contexto

PROBLEMA QUE RESUELVE:
---------------------
La palabra "banco" tiene m√∫ltiples significados:
  1. "Voy al banco a sacar dinero" (instituci√≥n financiera)
  2. "Me siento en el banco del parque" (asiento)

Word2Vec: Ambos "banco" usan el MISMO vector ‚ùå
BERT: Cada contexto usa VECTORES DIFERENTES ‚úì

OTRO EJEMPLO: "PLANTA"
---------------------
Frase 1: "La planta de tomates necesita agua"
         ‚Üí "planta" = ser vivo que crece (VECTOR A)

Frase 2: "La planta de fabricaci√≥n cerrar√° ma√±ana"
         ‚Üí "planta" = f√°brica/instalaci√≥n (VECTOR B)

Frase 3: "El atleta sinti√≥ dolor en la planta del pie"
         ‚Üí "planta" = parte del cuerpo (VECTOR C)

VECTOR A ‚â† VECTOR B ‚â† VECTOR C

¬°Cada uno es diferente porque el CONTEXTO es diferente!

¬øC√ìMO FUNCIONA?
---------------
1. BERT analiza la ORACI√ìN COMPLETA (antes y despu√©s de la palabra)
2. Usa REDES NEURONALES para entender el contexto
3. Genera un vector √öNICO para esa palabra EN ESE CONTEXTO
4. Resultado: Mejor comprensi√≥n del significado real

¬øPOR QU√â ES TAN POTENTE?
------------------------
‚úì Entiende m√∫ltiples significados (polisemia)
‚úì Entiende matices y contexto
‚úì Usa informaci√≥n bidireccional (mira antes Y despu√©s)
‚úì Pre-entrenado en MILLONES de textos
‚úì Base de muchos modelos modernos (ChatGPT, etc.)

VENTAJAS SOBRE WORD2VEC:
------------------------
Word2Vec:
  - R√°pido
  - Simple
  - Pero: no contextual

BERT:
  - Lento (necesita procesamiento complejo)
  - Complejo
  - Pero: contextual, m√°s preciso, m√°s inteligente ‚ú®

DESVENTAJAS:
----------
- M√°s lento que Word2Vec
- Requiere m√°s poder computacional
- Necesita librer√≠as especiales (transformers)
- Requiere descargar un modelo pre-entrenado

NOTA: En este archivo usamos BERT en espa√±ol (dccuchile/bert-base-spanish-wwm-cased)
"""

import torch
import numpy as np
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity


def cargar_bert_modelo():
    """
    Carga el modelo BERT pre-entrenado en espa√±ol
    
    Este modelo fue entrenado con millones de textos en espa√±ol,
    por lo que ya "entiende" el idioma espa√±ol
    """
    print("\n" + "="*80)
    print("CARGANDO BERT (Puede tardar unos segundos en la primera ejecuci√≥n)...")
    print("="*80)
    
    print("\nüì¶ Descargando modelo pre-entrenado...")
    print("   (dccuchile/bert-base-spanish-wwm-cased)")
    
    try:
        # Cargar tokenizador
        # El tokenizador convierte palabras en tokens que BERT entiende
        tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')
        
        # Cargar modelo
        # El modelo genera los vectores
        model = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')
        
        print("‚úÖ Modelo cargado exitosamente!")
        return tokenizer, model
    
    except Exception as e:
        print(f"\n‚ùå Error al cargar BERT: {e}")
        print("   Soluci√≥n: Instala las dependencias con:")
        print("   pip install transformers torch")
        return None, None


def obtener_embedding_contextual(text, target_word, tokenizer, model):
    """
    Funci√≥n para obtener el embedding (vector) de una palabra en un contexto espec√≠fico
    
    Par√°metros:
        text: la oraci√≥n completa (contexto)
        target_word: la palabra cuyo vector queremos
        tokenizer: tokenizador BERT
        model: modelo BERT
    
    Retorna:
        vector numpy que representa la palabra en ese contexto
    """
    
    # PASO 1: Tokenizar la oraci√≥n
    # Esto convierte la oraci√≥n en tokens que BERT entiende
    print(f"\nüìù Analizando: '{text}'")
    print(f"   Palabra objetivo: '{target_word}'")
    
    inputs = tokenizer(text, return_tensors="pt")
    
    # PASO 2: Procesar con BERT (sin guardar gradientes = faster)
    # torch.no_grad() dice: "No necesito calcular derivadas, solo predicciones"
    print("   ü§ñ Procesando con BERT...")
    
    with torch.no_grad():
        # output_hidden_states=True: queremos ver todos los estados internos
        outputs = model(**inputs, output_hidden_states=True)
    
    # PASO 3: Extraer los estados ocultos (las capas internas de BERT)
    # BERT tiene 12 capas, cada una representa diferentes niveles de informaci√≥n
    hidden_states = outputs.hidden_states
    
    # PASO 4: Convertir tokens a palabras para entender qu√© pas√≥
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    print(f"   Tokens generados por BERT: {tokens}")
    
    # PASO 5: Encontrar d√≥nde est√° la palabra que buscamos
    try:
        target_word_index = tokens.index(target_word)
        print(f"   ‚úì Palabra '{target_word}' encontrada en posici√≥n {target_word_index}")
    except ValueError:
        print(f"   ‚ùå Palabra '{target_word}' no encontrada como token √∫nico")
        print(f"      Nota: BERT puede dividir palabras en sub-tokens")
        return None
    
    # PASO 6: Obtener los embeddings de las √öLTIMAS 4 CAPAS de BERT
    # Las √∫ltimas capas contienen informaci√≥n m√°s contextual y sem√°ntica
    # (las primeras capas son m√°s b√°sicas, como reconocer caracteres)
    last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]
    
    # Sumar los embeddings de las 4 √∫ltimas capas
    # Esto combina informaci√≥n de diferentes niveles de comprensi√≥n
    token_embeddings = torch.stack(last_four_layers).sum(0)
    
    # Extraer el embedding de nuestra palabra espec√≠fica
    word_embedding = token_embeddings[0][target_word_index]
    
    # Convertir a numpy (formato que podemos usar f√°cilmente)
    return word_embedding.numpy()


def demostrar_contextualidad(tokenizer, model):
    """
    Demuestra c√≥mo BERT genera VECTORES DIFERENTES para la misma palabra
    en contextos diferentes (¬°esto es lo m√°gico!)
    """
    print("\n" + "="*80)
    print("DEMOSTRANDO LA CONTEXTUALIDAD DE BERT")
    print("="*80)
    
    print("\n‚ú® EXPERIMENTO: La palabra 'planta' en 3 contextos diferentes")
    print("="*80)
    
    # Tres oraciones con la palabra "planta" pero con significados diferentes
    oraciones = {
        'ser_vivo': "La planta de tomates que sembramos en el jard√≠n necesita mucho sol y agua para crecer",
        'fabrica': "La planta de ensamblaje de coches en la zona industrial tuvo que cerrar por falta de suministros",
        'cuerpo': "El atleta sinti√≥ un dolor agudo en toda la planta del pie despu√©s de correr la marat√≥n"
    }
    
    # Obtener los vectores para "planta" en cada contexto
    embeddings = {}
    
    for contexto, oracion in oraciones.items():
        print(f"\n{'='*80}")
        print(f"CONTEXTO: {contexto.upper()}")
        print(f"{'='*80}")
        
        embedding = obtener_embedding_contextual(oracion, 'planta', tokenizer, model)
        embeddings[contexto] = embedding
        
        if embedding is not None:
            print(f"   Vector generado: {embedding[:5]}... (primeros 5 valores de 768)")
            print(f"   Tama√±o del vector: {len(embedding)} dimensiones")
    
    # PASO 2: Comparar los vectores con similitud del coseno
    print("\n" + "="*80)
    print("COMPARANDO LOS VECTORES (similitud del coseno)")
    print("="*80)
    
    if all(v is not None for v in embeddings.values()):
        # Calcular similitudes
        sim_vivo_fabrica = cosine_similarity(
            [embeddings['ser_vivo']], 
            [embeddings['fabrica']]
        )[0][0]
        
        sim_vivo_cuerpo = cosine_similarity(
            [embeddings['ser_vivo']], 
            [embeddings['cuerpo']]
        )[0][0]
        
        sim_fabrica_cuerpo = cosine_similarity(
            [embeddings['fabrica']], 
            [embeddings['cuerpo']]
        )[0][0]
        
        print(f"\nüìä RESULTADOS:")
        print(f"\n   Ser Vivo vs. F√°brica:  {sim_vivo_fabrica:.4f}")
        print("   ‚Üì Interpretaci√≥n:")
        print("      Valores bajos (< 0.5) = contextos completamente diferentes")
        print("      BERT entiende que 'planta' significa cosas distintas")
        
        print(f"\n   Ser Vivo vs. Cuerpo:   {sim_vivo_cuerpo:.4f}")
        print("   ‚Üì Interpretaci√≥n:")
        print("      Ambas son 'seres vivos' pero diferentes")
        
        print(f"\n   F√°brica vs. Cuerpo:    {sim_fabrica_cuerpo:.4f}")
        print("   ‚Üì Interpretaci√≥n:")
        print("      Contextos muy diferentes")
        
        print("\n‚úÖ CONCLUSI√ìN:")
        print("   Los vectores para 'planta' son DIFERENTES en cada contexto")
        print("   ¬°Esto demuestra que BERT es CONTEXTUAL!")
        print("   No usa el mismo vector siempre, sino que adapta seg√∫n el contexto")


def comparacion_final():
    """
    Comparaci√≥n final de todas las t√©cnicas
    """
    print("\n" + "="*80)
    print("COMPARACI√ìN FINAL: BoW vs TF-IDF vs Word2Vec vs BERT")
    print("="*80)
    
    tecnicas = {
        'Bag of Words': {
            'Qu√© hace': 'Cuenta frecuencia de palabras',
            'Output': '[0, 1, 0, 1, 0, 1, ...]  (0s y 1s)',
            'Contextual': '‚ùå No',
            'Rapidez': '‚ö°‚ö°‚ö° Muy r√°pido',
            'Complejidad': 'üü¢ Muy simple',
            'Mejor para': 'Tareas simples, an√°lisis r√°pido'
        },
        'TF-IDF': {
            'Qu√© hace': 'Pondera palabras por importancia',
            'Output': '[0.1, 0.34, 0.05, ...]  (n√∫meros decimales)',
            'Contextual': '‚ùå No',
            'Rapidez': '‚ö°‚ö°‚ö° Muy r√°pido',
            'Complejidad': 'üü¢ Simple',
            'Mejor para': 'B√∫squeda de documentos, clasificaci√≥n'
        },
        'Word2Vec': {
            'Qu√© hace': 'Aprende relaciones entre palabras',
            'Output': '[0.23, -0.54, 0.81, ...]  (valores significativos)',
            'Contextual': '‚ùå No (mismo vector siempre)',
            'Rapidez': '‚ö°‚ö° R√°pido',
            'Complejidad': 'üü° Media',
            'Mejor para': 'Similitud, analog√≠as, entrada para redes'
        },
        'BERT': {
            'Qu√© hace': 'Entiende contexto y significado profundo',
            'Output': '[0.12, 0.89, -0.45, ...]  (768 valores contextuales)',
            'Contextual': '‚úÖ S√≠ (vectores diferentes por contexto)',
            'Rapidez': 'üê¢ Lento',
            'Complejidad': 'üî¥ Muy complejo',
            'Mejor para': 'Tareas avanzadas, clasificaci√≥n, an√°lisis profundo'
        }
    }
    
    for tecnica, info in tecnicas.items():
        print(f"\n{'='*80}")
        print(f"üîπ {tecnica}")
        print(f"{'='*80}")
        for clave, valor in info.items():
            print(f"   {clave:20}: {valor}")
    
    print("\n" + "="*80)
    print("üìà EVOLUCI√ìN DEL PODER")
    print("="*80)
    print("\nBoW ‚Üí TF-IDF ‚Üí Word2Vec ‚Üí BERT")
    print("     ‚Üì         ‚Üì           ‚Üì")
    print("   Mejora   Mejora     üöÄ REVOLUCI√ìN")
    print("\nCada t√©cnica es m√°s poderosa pero tambi√©n m√°s lenta/compleja")


if __name__ == "__main__":
    # Cargar modelo BERT
    tokenizer, model = cargar_bert_modelo()
    
    if tokenizer and model:
        # Demostrar contextualidad
        demostrar_contextualidad(tokenizer, model)
        
        # Comparaci√≥n final
        comparacion_final()
        
        print("\n" + "="*80)
        print("‚úÖ BERT FINALIZADO")
        print("="*80)
    else:
        print("\n‚ùå No se pudo completar el ejercicio de BERT")
