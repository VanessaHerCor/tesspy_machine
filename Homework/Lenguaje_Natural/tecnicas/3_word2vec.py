"""
=============================================================================
T√âCNICA 3: WORD2VEC - EMBEDDINGS EST√ÅTICOS (CONTEXTO SIMPLE)
=============================================================================

¬øQU√â ES?
--------
Word2Vec crea EMBEDDINGS: vectores que capturan el SIGNIFICADO de las palabras.
No solo cuenta frecuencias, sino que APRENDE relaciones entre palabras.

DIFERENCIA CON BoW Y TF-IDF:
----------------------------
  BoW/TF-IDF: "¬øCu√°ntas veces aparece cada palabra?"
              ‚Üí Resultado: matriz de n√∫meros sin mucho significado

  Word2Vec: "¬øCu√°l es el CONTEXTO en el que aparece la palabra?"
            "¬øQu√© palabras aparecen cerca de ella?"
            ‚Üí Resultado: vectores que capturan el SIGNIFICADO
            
IDEA PRINCIPAL:
---------------
"Las palabras que aparecen en contextos similares tienen significados similares"

Ejemplos:
  ‚Ä¢ "rey" y "reina" aparecen cerca de: hombre, mujer, corona, poder
    ‚Üí Sus vectores ser√°n SIMILARES
  
  ‚Ä¢ "perro" y "gato" aparecen con: animal, mascotas, juegan, comen
    ‚Üí Sus vectores ser√°n SIMILARES
  
  ‚Ä¢ "perro" y "contenedor" no aparecen en contextos similares
    ‚Üí Sus vectores ser√°n DIFERENTES

VENTAJA IMPORTANTE:
-------------------
Word2Vec es EST√ÅTICO: cada palabra tiene UN SOLO vector (no cambia con contexto)
Esto es simple pero limitado (lo veremos m√°s tarde con BERT)

C√ìMO FUNCIONA (muy simplificado):
---------------------------------
1. Toma un corpus de textos
2. Para cada palabra, mira las palabras VECINAS (dentro de una ventana)
3. Aprende: "¬øSi veo esta palabra, qu√© otras palabras probablemente vea cerca?"
4. Codifica eso en un vector de n√∫meros
5. Palabras que co-aparecen frecuentemente tienen vectores SIMILARES

VECTOR RESULTANTE:
------------------
En lugar de:
  "perro" ‚Üí [0, 1, 0, 0, 0, 1, 0, 0, 0, ...]  (BoW, solo 0s y 1s)

Word2Vec crea:
  "perro" ‚Üí [0.23, -0.54, 0.81, 0.12, -0.33, ...]  (valores continuos con significado)

CARACTER√çSTICAS M√ÅGICAS:
-----------------------
‚Ä¢ Analog√≠as: rey - hombre + mujer ‚âà reina
  (La matem√°tica de los vectores captura las relaciones!)
  
‚Ä¢ Similitud: cosine_similarity(vector_perro, vector_gato) ‚âà 0.85
  (Vectores similares = palabras relacionadas)
"""

import numpy as np
from gensim.models import Word2Vec
from sklearn.metrics.pairwise import cosine_similarity


def explicar_word2vec():
    """
    Funci√≥n que EXPLICA y DEMUESTRA c√≥mo funciona Word2Vec
    """
    print("\n" + "="*80)
    print("WORD2VEC - EMBEDDINGS DE PALABRAS")
    print("="*80)
    
    # PASO 1: Preparar corpus
    # Nota: Word2Vec necesita el corpus como LISTA DE LISTAS de palabras
    print("\nüìö PREPARANDO CORPUS...")
    corpus = [
        ['rey', 'es', 'un', 'hombre', 'poderoso'],
        ['reina', 'es', 'una', 'mujer', 'poderosa'],
        ['mujer', 'es', 'inteligente'],
        ['el', 'rey', 'gobierna', 'el', 'reino'],
        ['la', 'reina', 'gobierna', 'el', 'reino']
    ]
    
    print("Oraciones del corpus:")
    for i, sent in enumerate(corpus, 1):
        print(f"   {i}. {' '.join(sent)}")
    
    # PASO 2: Entrenar Word2Vec
    print("\nü§ñ ENTRENANDO WORD2VEC...")
    print("   (Analizando relaciones entre palabras en el corpus...)")
    
    # Par√°metros:
    # vector_size: cu√°ntos n√∫meros en cada vector (10 = 10 dimensiones)
    # window: cu√°ntas palabras a cada lado mirar (5 = mira 5 palabras antes y despu√©s)
    # min_count: ignorar palabras que aparecen menos de 1 vez
    model = Word2Vec(
        sentences=corpus,
        vector_size=10,  # Vectores de 10 dimensiones
        window=5,        # Ventana de contexto de 5 palabras
        min_count=1,     # Incluir todas las palabras
        workers=4,       # Usar 4 procesadores
        epochs=100       # Entrenar 100 veces (m√°s entrenamiento = mejor)
    )
    
    print("‚úÖ Entrenamiento completado!")
    
    # PASO 3: Explorar los vectores
    print("\nüìä VECTORES APRENDIDOS:")
    print("\n   Vector para 'rey' (10 n√∫meros):")
    vector_rey = model.wv['rey']
    print(f"   {vector_rey}")
    print("   (Estos n√∫meros capturan el 'significado' de 'rey' seg√∫n el corpus)")
    
    print("\n   Vector para 'reina':")
    vector_reina = model.wv['reina']
    print(f"   {vector_reina}")
    
    print("\n   Vector para 'contenedor' (palabra no en el corpus):")
    try:
        vector_contenedor = model.wv['contenedor']
        print(f"   {vector_contenedor}")
    except KeyError:
        print("   ‚ùå Error: 'contenedor' no est√° en el vocabulario")
        print("      (solo puede representar palabras que vio durante el entrenamiento)")
    
    # PASO 4: Similitud entre palabras
    print("\nüîç SIMILITUD ENTRE PALABRAS (cosine similarity):")
    print("   (1.0 = id√©nticas, 0.0 = completamente diferentes)")
    
    # Calcular similitudes
    sim_rey_reina = model.wv.similarity('rey', 'reina')
    sim_rey_hombre = model.wv.similarity('rey', 'hombre')
    sim_rey_mujer = model.wv.similarity('rey', 'mujer')
    sim_rey_gato = model.wv.similarity('rey', 'inteligente')
    
    print(f"\n   'rey' vs 'reina': {sim_rey_reina:.4f}")
    print("   ‚úì MUY SIMILAR (ambos son monarcas)")
    
    print(f"\n   'rey' vs 'hombre': {sim_rey_hombre:.4f}")
    print("   ‚úì SIMILAR (rey es hombre)")
    
    print(f"\n   'rey' vs 'mujer': {sim_rey_mujer:.4f}")
    print("   ‚úó MENOS SIMILAR (rey es t√≠picamente hombre, no mujer)")
    
    print(f"\n   'rey' vs 'inteligente': {sim_rey_gato:.4f}")
    print("   ‚úó POCO SIMILAR (caracter√≠sticas diferentes)")
    
    # PASO 5: Palabras m√°s similares
    print("\nüéØ PALABRAS M√ÅS SIMILARES A 'rey':")
    similares_rey = model.wv.most_similar('rey', topn=3)
    for palabra, similitud in similares_rey:
        print(f"   ‚Ä¢ {palabra}: {similitud:.4f}")
    
    print("\nüéØ PALABRAS M√ÅS SIMILARES A 'mujer':")
    similares_mujer = model.wv.most_similar('mujer', topn=3)
    for palabra, similitud in similares_mujer:
        print(f"   ‚Ä¢ {palabra}: {similitud:.4f}")
    
    return model


def demostrar_analogias(model):
    """
    Funci√≥n que DEMUESTRA la propiedad m√°s m√°gica de Word2Vec: las ANALOG√çAS
    
    Ejemplo: rey - hombre + mujer ‚âà reina
    
    Explicaci√≥n:
    - Tomamos el vector de 'rey'
    - Le restamos el vector de 'hombre' (quitamos la caracter√≠stica "masculino")
    - Le sumamos el vector de 'mujer' (a√±adimos la caracter√≠stica "femenino")
    - ¬°El resultado es el vector m√°s cercano a 'reina'!
    """
    print("\n" + "="*80)
    print("ANALOG√çAS EN WORD2VEC (¬°LA PARTE M√ÅGICA!)")
    print("="*80)
    
    print("\n‚ú® EJEMPLO: Rey - Hombre + Mujer = ?")
    print("   Pregunta: ¬øSi rey es a hombre como X es a mujer?")
    print("   Respuesta esperada: REINA")
    
    print("\n   Explicaci√≥n matem√°tica:")
    print("   vector('rey') - vector('hombre') + vector('mujer') ‚âà vector('reina')")
    print("   Porque:")
    print("      ‚Ä¢ vector('rey') - vector('hombre') = caracter√≠stica 'royal'")
    print("      ‚Ä¢ + vector('mujer') = aplicar 'royal' a una mujer")
    print("      ‚Ä¢ = vector('reina')")
    
    try:
        resultado = model.wv.most_similar(
            positive=['rey', 'mujer'],  # Sumar estos vectores
            negative=['hombre'],         # Restar este vector
            topn=3
        )
        
        print("\nüéØ RESULTADO DE LA ANALOG√çA:")
        print("   Palabras m√°s cercanas a (rey - hombre + mujer):")
        for palabra, similitud in resultado:
            print(f"   ‚Ä¢ {palabra}: {similitud:.4f}")
            if palabra == 'reina':
                print("     ‚ú® ¬°CORRECTO! Es la respuesta que esper√°bamos!")
    
    except Exception as e:
        print(f"\n‚ö†Ô∏è  No se pudo completar la analog√≠a: {e}")
        print("    (Esto puede ocurrir si el corpus es muy peque√±o)")


def comparar_con_bow():
    """
    Funci√≥n que COMPARA Word2Vec con BoW para mostrar las diferencias
    """
    print("\n" + "="*80)
    print("COMPARACI√ìN: BoW vs TF-IDF vs Word2Vec")
    print("="*80)
    
    print("\nPara la palabra 'rey':")
    
    print("\n1Ô∏è‚É£  BOW (Bag of Words):")
    print("    Resultado: [0, 1, 0, 0, 0, 1, 0, 0, 0, ...]")
    print("    ‚Üí Solo cuenta: ¬øaparece o no? (0 o 1)")
    print("    ‚Üí No hay informaci√≥n sobre significado")
    
    print("\n2Ô∏è‚É£  TF-IDF:")
    print("    Resultado: [0, 0.34, 0, 0, 0, 0.67, 0, 0, 0, ...]")
    print("    ‚Üí Cuenta frecuencia y rareza")
    print("    ‚Üí Pondera palabras m√°s importantes")
    print("    ‚Üí Pero sigue siendo un conteo, no significado real")
    
    print("\n3Ô∏è‚É£  WORD2VEC:")
    print("    Resultado: [0.23, -0.54, 0.81, 0.12, -0.33, 0.19, ...]")
    print("    ‚Üí N√∫meros que representan SIGNIFICADO")
    print("    ‚Üí Captura relaciones con otras palabras")
    print("    ‚Üí Permite calcular similitud y analog√≠as")
    print("    ‚Üí Mucho m√°s poderoso para tareas de IA")
    
    print("\n‚úÖ VENTAJAS DE WORD2VEC:")
    print("    ‚Ä¢ Captura significado sem√°ntico")
    print("    ‚Ä¢ Permite similitud entre palabras")
    print("    ‚Ä¢ Permite analog√≠as")
    print("    ‚Ä¢ Input ideal para redes neuronales")
    
    print("\n‚ö†Ô∏è  DESVENTAJAS DE WORD2VEC:")
    print("    ‚Ä¢ Cada palabra tiene UN SOLO vector (no contextual)")
    print("    ‚Ä¢ 'banco' = instituci√≥n financiera o asiento")
    print("      ‚Üí Ambos significados tienen el MISMO vector")
    print("    ‚Ä¢ No distingue por contexto")
    print("    ‚Ä¢ Necesita entrenamiento previo")


if __name__ == "__main__":
    # Entrenar y explicar Word2Vec
    model = explicar_word2vec()
    
    # Demostrar analog√≠as
    demostrar_analogias(model)
    
    # Comparar con otras t√©cnicas
    comparar_con_bow()
    
    print("\n" + "="*80)
    print("‚úÖ WORD2VEC FINALIZADO")
    print("="*80)
    print("\nüí° PR√ìXIMO PASO: BERT (embeddings CONTEXTUALES)")
    print("   BERT usa el mismo principio pero con inteligencia adicional:")
    print("   ¬°Cada palabra obtiene vectores DIFERENTES seg√∫n el contexto!")
