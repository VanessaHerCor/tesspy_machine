# INSTALAR ANTES
# pip install -U "numpy>=2" "tenacity>=9,<10"
# pip install torch langchain langchain-community langchain-text-splitters transformers sentence-transformers faiss-cpu langchain-huggingface
# pip install pypdf


# ============================================
# CHATBOT CON IA USANDO LANGCHAIN Y HUGGINGFACE
# ============================================
# Este script implementa un chatbot que responde preguntas
# basado en documentos PDF locales usando un modelo de IA

import os
import glob
import torch
from pathlib import Path

# ============= IMPORTACIONES NECESARIAS =============

# Herramientas para cargar y procesar PDFs
from langchain_community.document_loaders import PyPDFLoader

# Herramientas para dividir documentos en fragmentos pequeÃ±os
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Herramientas para crear representaciones vectoriales (embeddings) del texto
from langchain_community.embeddings import HuggingFaceEmbeddings

# FAISS es una librerÃ­a para bÃºsqueda rÃ¡pida de vectores similares
from langchain_community.vectorstores import FAISS

# Pipeline para usar modelos de IA localmente
from langchain_huggingface import HuggingFacePipeline

# Modelos y tokenizadores de Hugging Face
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline


# ============= PASO 0: CONFIGURAR RUTAS Y CARPETAS =============

# Definir la carpeta donde estÃ¡n los PDFs (LOCAL, no en Colab)
pdf_folder_path = Path(__file__).parent / "PDF_PSY"

# Carpeta donde se guardarÃ¡n los embeddings (para reutilizarlos)
embedding_folder_path = Path(__file__).parent / "embedding_storage"

print("=" * 60)
print("PASO 0: VERIFICANDO CARPETAS")
print("=" * 60)
print(f"ðŸ“‚ Carpeta de PDFs: {pdf_folder_path}")
print(f"ðŸ“‚ Carpeta de embeddings: {embedding_folder_path}")

# Crear la carpeta de embeddings si no existe
if not os.path.exists(embedding_folder_path):
    os.makedirs(embedding_folder_path)
    print(f"âœ… Carpeta de embeddings creada")


# ============= PASO 1: VERIFICAR CARPETA DE PDFS =============

print("\n" + "=" * 60)
print("PASO 1: VERIFICANDO CARPETA DE PDFS")
print("=" * 60)
print(f"Buscando PDFs en: {pdf_folder_path}")

# Verificar si la carpeta existe
if not os.path.exists(pdf_folder_path):
    print(f"âŒ Error: La carpeta {pdf_folder_path} no existe")
    print("Por favor, asegÃºrate de que tus PDFs estÃ©n en: Homework/Chatbot/PDF_PSY")
    exit()

# Buscar todos los archivos PDF en la carpeta
pdf_files = glob.glob(f"{pdf_folder_path}/*.pdf")

if not pdf_files:
    print(f"âš ï¸  No se encontraron PDFs en {pdf_folder_path}")
else:
    print(f"âœ… Se encontraron {len(pdf_files)} PDF(s):")
    for pdf in pdf_files:
        print(f"   - {os.path.basename(pdf)}")


# ============= PASO 2 Y 3: CARGAR Y PROCESAR PDFS O CARGAR EMBEDDINGS =============

print("\n" + "=" * 60)
print("PASO 2-3: PROCESANDO DOCUMENTOS")
print("=" * 60)

# Verificar si ya existen embeddings guardados
if os.path.exists(embedding_folder_path / "index.faiss"):
    print("âš¡ Se detectaron embeddings guardados previamente")
    print("ðŸ”„ Cargando embeddings desde cachÃ© (MUCHO MÃS RÃPIDO)...")
    
    # Cargar el modelo de embeddings
    embeddings_local = HuggingFaceEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2'
    )
    
    # Cargar la base de datos de vectores guardada
    vectorstore = FAISS.load_local(
        str(embedding_folder_path),
        embeddings_local,
        allow_dangerous_deserialization=True
    )
    print("âœ… Embeddings cargados desde cachÃ© (Â¡sin regenerar!)")
    
else:
    # Si no existen embeddings, generarlos
    print("ðŸ“ Generando embeddings por primera vez (esto toma tiempo)...")
    print("   Las prÃ³ximas ejecuciones serÃ¡n MUCHO mÃ¡s rÃ¡pidas ðŸš€\n")
    
    # ---- CARGAR PDFS ----
    print("ðŸ“„ Cargando PDFs...")
    all_pages = []
    
    for pdf_file in pdf_files:
        print(f"   - {os.path.basename(pdf_file)}", end=" ")
        
        try:
            loader = PyPDFLoader(pdf_file)
            pages = loader.load()
            all_pages.extend(pages)
            print(f"âœ… ({len(pages)} pÃ¡ginas)")
        except Exception as e:
            print(f"âŒ Error: {e}")
    
    print(f"\nâœ… Total de pÃ¡ginas cargadas: {len(all_pages)}")
    
    # ---- DIVIDIR EN FRAGMENTOS ----
    print("\nðŸ“Š Dividiendo documentos en fragmentos...")
    text_split = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=40
    )
    docs = text_split.split_documents(all_pages)
    print(f"âœ… {len(docs)} fragmentos creados")
    
    # ---- CREAR EMBEDDINGS ----
    print("\nðŸ”¢ Generando vectores (embeddings)...")
    print("   Modelo: sentence-transformers/all-MiniLM-L6-v2")
    
    embeddings_local = HuggingFaceEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2'
    )
    
    vectorstore = FAISS.from_documents(docs, embeddings_local)
    
    # ---- GUARDAR EMBEDDINGS PARA LA PRÃ“XIMA VEZ ----
    print("\nðŸ’¾ Guardando embeddings en cachÃ©...")
    vectorstore.save_local(str(embedding_folder_path))
    print(f"âœ… Embeddings guardados en: {embedding_folder_path}")
    print("   âš¡ La prÃ³xima ejecuciÃ³n serÃ¡ INSTANTÃNEA")


# ============= PASO 4: CREAR EL RETRIEVER =============

print("\n" + "=" * 60)
print("PASO 4: CONFIGURANDO RETRIEVER")
print("=" * 60)
print("""
Â¿QuÃ© es el retriever?
- Busca los documentos mÃ¡s relevantes para cada pregunta
- Usa los vectores para encontrar informaciÃ³n relacionada
""")

# Configurar el recuperador
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
print("âœ… Retriever listo")

# Probar el retriever
print("\nðŸ§ª Prueba del retriever:")
pregunta_prueba = "Â¿QuÃ© es la depresiÃ³n?"
docs_relevantes = retriever.invoke(pregunta_prueba)
print(f"   Pregunta: '{pregunta_prueba}'")
print(f"   âœ… Se encontraron {len(docs_relevantes)} documentos relevantes")


# ============= PASO 5: CARGAR EL MODELO DE LENGUAJE =============

print("\n" + "=" * 60)
print("PASO 5: CARGANDO MODELO DE LENGUAJE (LLM)")
print("=" * 60)
print("""
Modelo: Qwen/Qwen3-0.6B
- Ligero: Solo 0.6 mil millones de parÃ¡metros
- RÃ¡pido: Funciona bien en CPU
- Eficiente: Bajo consumo de memoria
""")

model_id = 'Qwen/Qwen3-0.6B'
print(f"â³ Cargando {model_id}...")

try:
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    print("âœ… Tokenizador cargado")
    
    # Cargar el modelo sin device_map (funciona mejor en CPU)
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        dtype=torch.float16
    )
    print("âœ… Modelo cargado")
    
    # Configurar el pipeline con parÃ¡metros para evitar repeticiones
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,          # Limitar a 256 tokens para respuestas concisas
        do_sample=True,              # Usar sampling para respuestas variadas
        temperature=0.7,             # Temperatura (0.7 = balance entre creatividad y coherencia)
        top_p=0.9,                   # Nucleus sampling
        repetition_penalty=1.2,      # Penalizar repeticiones (IMPORTANTE)
        eos_token_id=tokenizer.eos_token_id
    )
    
    llm_local = HuggingFacePipeline(pipeline=pipe)
    
    print("âœ… Pipeline de generaciÃ³n listo")
    print("\n" + "=" * 60)
    print("ðŸŽ‰ Â¡CHATBOT COMPLETAMENTE CONFIGURADO!")
    print("=" * 60)
    
except Exception as e:
    print(f"âŒ Error: {e}")
    exit()


# ============= PASO 6: FUNCIÃ“N PARA HACER PREGUNTAS =============

def hacer_pregunta(pregunta):
    """
    FunciÃ³n para hacer preguntas al chatbot.
    
    Pasos:
    1. El retriever busca documentos relevantes
    2. Se construye un prompt con contexto
    3. El LLM genera una respuesta
    
    Args:
        pregunta (str): La pregunta del usuario
    
    Returns:
        str: La respuesta generada por la IA
    """
    print(f"\nðŸ’­ Buscando informaciÃ³n relevante...")
    
    # Obtener documentos relevantes
    docs_contexto = retriever.invoke(pregunta)
    
    # Preparar el contexto
    contexto = "\n".join([
        f"[Documento {i+1}]\n{doc.page_content}"
        for i, doc in enumerate(docs_contexto)
    ])
    
    # Crear un prompt mejor estructurado
    prompt = f"""You are a helpful psychology expert assistant. Based on the provided documents, answer the user's question clearly and accurately.

Context information:
{contexto}

User question: {pregunta}

Answer based only on the context provided above. Be concise and direct:"""
    
    print("â³ Generando respuesta...")
    
    # Generar respuesta
    try:
        respuesta = llm_local.invoke(prompt)
        # Limpiar la respuesta de repeticiones obvias
        return respuesta[:800]  # Limitar longitud
    except Exception as e:
        return f"Error al generar respuesta: {e}"


# ============= PASO 7: MENÃš INTERACTIVO =============

def menu_principal():
    """
    MenÃº interactivo para hacer preguntas al chatbot
    """
    print("\n\n" + "=" * 60)
    print("CHATBOT PSICOLOGÃA - MENÃš INTERACTIVO")
    print("=" * 60)
    print("""
INSTRUCCIONES:
- Escribe tu pregunta sobre psicologÃ­a
- El chatbot buscarÃ¡ en los PDFs y responderÃ¡
- Escribe 'salir' para terminar
""")
    
    while True:
        try:
            # Obtener pregunta del usuario
            pregunta = input("\nðŸ“ Tu pregunta: ").strip()
            
            # Verificar si el usuario quiere salir
            if pregunta.lower() in ['salir', 'exit', 'quit', 'no']:
                print("\nðŸ‘‹ Â¡Hasta luego! Gracias por usar el chatbot.")
                break
            
            # Validar que no estÃ© vacÃ­o
            if not pregunta:
                print("âš ï¸  Por favor escribe una pregunta vÃ¡lida")
                continue
            
            # Hacer la pregunta
            respuesta = hacer_pregunta(pregunta)
            
            print("\n" + "-" * 60)
            print("ðŸ¤– RESPUESTA DEL CHATBOT:")
            print("-" * 60)
            print(respuesta)
            print("-" * 60)
            
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Chatbot terminado.")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")
            continue


# ============= PROGRAMA PRINCIPAL =============

if __name__ == "__main__":
    # Ejemplo de uso automÃ¡tico (comentado)
    print("\n\n" + "=" * 60)
    print("EJEMPLO DE USO")
    print("=" * 60)
    
    pregunta_ejemplo = "Â¿QuÃ© es la depresiÃ³n?"
    print(f"\nðŸ’¬ Pregunta: {pregunta_ejemplo}")
    respuesta = hacer_pregunta(pregunta_ejemplo)
    print(f"\nðŸ¤– Respuesta:\n{respuesta}\n")
    
    # MenÃº interactivo
    print("\nAhora puedes hacer tus propias preguntas:")
    menu_principal()