# ============================================================================
# CHATBOT DE PSICOLOG√çA - PROYECTO FINAL
# Basado en LangChain y Sistema RAG (Retrieval-Augmented Generation)
# ============================================================================

# PASO 1: IMPORTAR LIBRER√çAS NECESARIAS
# ============================================================================

import os
import glob  # Para buscar archivos PDF
from pathlib import Path

# Librer√≠as de LangChain - el framework principal para construir chatbots
from langchain_community.document_loaders import PyPDFLoader  # Carga archivos PDF
from langchain_text_splitters import RecursiveCharacterTextSplitter  # Divide textos en fragmentos
from langchain_community.vectorstores import FAISS  # Base de datos de vectores (embeddings)
from langchain_huggingface import HuggingFaceEmbeddings  # Convierte texto a vectores (versi√≥n actualizada)
from langchain_community.llms import HuggingFacePipeline  # Pipeline local de HuggingFace

# Para usar modelos locales
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline  # Modelos locales
import torch  # Para GPU (si est√° disponible)

# ============================================================================
# PASO 2: CONFIGURAR LA RUTA A LOS PDFs
# ============================================================================

# Indicar d√≥nde est√°n guardados los archivos PDF
PDF_FOLDER = "PDF_PSY"  # Carpeta con los PDFs

# Validar que la carpeta existe
if not os.path.exists(PDF_FOLDER):
    print(f"‚ùå ERROR: La carpeta '{PDF_FOLDER}' no existe.")
    print("Por favor, crea una carpeta 'PDF_PSY' en la misma ubicaci√≥n que este archivo.")
    exit()

# Buscar todos los archivos PDF en la carpeta
pdf_files = glob.glob(os.path.join(PDF_FOLDER, "*.pdf"))

if not pdf_files:
    print(f"‚ùå ERROR: No hay archivos PDF en la carpeta '{PDF_FOLDER}'")
    exit()

print(f"‚úÖ Se encontraron {len(pdf_files)} archivos PDF")
print("Archivos cargados:")
for pdf in pdf_files:
    print(f"  - {os.path.basename(pdf)}")

# ============================================================================
# PASO 3: CARGAR Y PROCESAR LOS PDFs
# ============================================================================
print("\nüìÑ Cargando documentos PDF...")

# Lista para almacenar todos los documentos
all_documents = []

# Cargar cada PDF
for pdf_file in pdf_files:
    try:
        loader = PyPDFLoader(pdf_file)
        documents = loader.load()
        all_documents.extend(documents)
        print(f"‚úÖ Cargado: {os.path.basename(pdf_file)} ({len(documents)} p√°ginas)")
    except Exception as e:
        print(f"‚ö†Ô∏è Error al cargar {pdf_file}: {e}")

print(f"\n‚úÖ Total de documentos cargados: {len(all_documents)}")

# ============================================================================
# PASO 4: DIVIDIR DOCUMENTOS EN FRAGMENTOS PEQUE√ëOS (CHUNKS)
# ============================================================================
print("\n‚úÇÔ∏è Dividiendo documentos en fragmentos...")

# Dividir el texto en fragmentos m√°s peque√±os (600 chars) para mejor comprensi√≥n
# Chunks m√°s peque√±os = mejor relevancia y menos confusi√≥n del modelo
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,       # Fragmentos m√°s peque√±os = mejor precisi√≥n
    chunk_overlap=100,    # Solapamiento menor para eficiencia
)

# Aplicar la divisi√≥n a todos los documentos
chunks = text_splitter.split_documents(all_documents)
print(f"‚úÖ Documentos divididos en {len(chunks)} fragmentos")

# ============================================================================
# PASO 5: CREAR EMBEDDINGS (VECTORES) DE LOS FRAGMENTOS
# ============================================================================
print("\nüß† Creando embeddings (vectores)...")

# Usar HuggingFace para crear embeddings - estos son GRATUITOS
# Los embeddings convierten texto en n√∫meros que representan el significado
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    # Este modelo funciona bien con textos en espa√±ol
)

# Nombre del archivo donde se guardar√°n los embeddings
EMBEDDINGS_PATH = "embedding_storage"

# Verificar si ya existen embeddings guardados (para no procesarlos de nuevo)
if os.path.exists(EMBEDDINGS_PATH):
    print(f"‚úÖ Encontrados embeddings guardados en '{EMBEDDINGS_PATH}'")
    print("Cargando embeddings... (esto es MUCHO m√°s r√°pido)")
    vector_store = FAISS.load_local(EMBEDDINGS_PATH, embeddings, allow_dangerous_deserialization=True)
    print("‚úÖ Embeddings cargados exitosamente")
else:
    # Crear la base de datos de vectores usando FAISS (muy r√°pido y eficiente)
    # Esto almacena todos los embeddings y permite b√∫squedas r√°pidas
    print("Primera vez: creando embeddings...")
    print("Esto puede tardar unos momentos...")
    vector_store = FAISS.from_documents(chunks, embeddings)
    
    # GUARDAR los embeddings para la pr√≥xima vez (igual que tu profesor)
    print(f"\nüíæ Guardando embeddings en '{EMBEDDINGS_PATH}' para uso futuro...")
    vector_store.save_local(EMBEDDINGS_PATH)
    print("‚úÖ Base de datos de vectores creada y guardada exitosamente")
    print("‚ö° La pr√≥xima vez cargar√° MUCHO m√°s r√°pido")

# ============================================================================
# PASO 6: CREAR EL RETRIEVER
# ============================================================================
print("\nüîç Configurando el retriever...")

# El retriever busca los fragmentos m√°s relevantes para cada pregunta
# search_kwargs={'k': 4} significa que traer√° los 4 fragmentos m√°s similares
retriever = vector_store.as_retriever(search_kwargs={"k": 4})

print("‚úÖ Retriever configurado (buscar√° los 4 documentos m√°s relevantes)")

# ============================================================================
# PASO 7: CONFIGURAR EL MODELO DE LENGUAJE (LLM)
# ============================================================================
print("\nü§ñ Configurando el modelo de lenguaje...")

# Vamos a usar un modelo GRATUITO de HuggingFace
# Puedes cambiar el modelo seg√∫n tus necesidades
# Modelos recomendados: "mistralai/Mistral-7B-Instruct-v0.2" o "meta-llama/Llama-2-7b-chat"

# ============================================================================
# PASO 7: CONFIGURAR EL MODELO DE LENGUAJE LOCAL (LLM)
# ============================================================================
print("\nü§ñ Configurando el modelo de lenguaje local...")
print("‚ö†Ô∏è Primera vez: descargar√° ~7GB (puede tardar 10-20 minutos)...")

llm = None  # Inicializar

try:
    # Usar Microsoft Phi-2: modelo poderoso optimizado
    # Con par√°metros ajustados para mejor rendimiento
    model_id = "microsoft/phi-2"
    
    print(f"üì• Descargando modelo: {model_id}")
    print("Este proceso solo ocurre la primera vez...")
    
    # Cargar el tokenizador
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Cargar el modelo (esto descargar√° ~7GB)
    # torch_dtype=torch.float16 lo hace m√°s peque√±o y r√°pido
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch.float16  # Usar 16-bit para usar menos memoria
    )
    
    # Crear el pipeline de generaci√≥n de texto con par√°metros optimizados
    # Estos par√°metros evitan repeticiones y generan respuestas coherentes
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=300,           # Respuestas moderadas (300 tokens)
        temperature=0.7,              # Variedad en generaci√≥n (evita monoton√≠a)
        top_p=0.9,                    # Nucleus sampling (variedad controlada)
        repetition_penalty=1.2,       # Penaliza repeticiones
        do_sample=True                # Muestreo para diversidad
    )
    
    # Envolver en HuggingFacePipeline para LangChain
    llm = HuggingFacePipeline(pipeline=pipe)
    
    print("‚úÖ Modelo de lenguaje configurado correctamente")
    print(f"‚úÖ Usando: {model_id}")
    
except Exception as e:
    print(f"‚ùå Error al cargar el modelo: {e}")
    print("Aseg√∫rate de tener:")
    print("  - 16GB+ RAM disponible")
    print("  - PyTorch instalado: pip install torch")
    print("  - Conexi√≥n a internet (para descargar el modelo)")
    llm = None

# ============================================================================
# PASO 8: CONFIGURACI√ìN LISTA
# ============================================================================
print("‚úÖ Todos los componentes est√°n listos")

# ============================================================================
# PASO 9: CREAR LA CADENA CONVERSACIONAL (MANUAL - compatible con LangChain nuevo)
# ============================================================================
print("\n‚õìÔ∏è Configurando el sistema de respuestas...")

qa_chain = None

if llm is not None:
    # En lugar de usar ConversationalRetrievalChain (que est√° deprecado),
    # vamos a implementar la l√≥gica manualmente pero M√ÅS compatible
    # Esto le permite al modelo recordar el historial
    
    print("‚úÖ Sistema de respuestas configurado")
    qa_chain = True  # Marcador simple de que est√° listo
else:
    print("‚ö†Ô∏è No se puede crear la cadena sin el modelo LLM")
    print("Verifica que el modelo se carg√≥ correctamente")

# ============================================================================
# PASO 10: VARIABLE PARA GUARDAR EL HISTORIAL DE CHAT
# ============================================================================

# El historial se guarda aqu√≠ para que la IA recuerde la conversaci√≥n
chat_history = []

# ============================================================================
# PASO 11: FUNCI√ìN PARA HACER PREGUNTAS AL CHATBOT
# ============================================================================

def hacer_pregunta(pregunta):
    """
    Funci√≥n para hacer una pregunta al chatbot con historial conversacional
    
    Args:
        pregunta (str): La pregunta que deseas hacer
    """
    global chat_history  # Usar el historial global
    
    print(f"\nüë§ Tu pregunta: {pregunta}")
    print("-" * 60)
    
    if qa_chain is None:
        print("‚ùå El chatbot no est√° disponible")
        return
    
    try:
        # PASO 1: Buscar documentos relevantes
        print("üîç Buscando informaci√≥n relevante...")
        docs_relevantes = retriever.invoke(pregunta)
        
        # PASO 2: Preparar el contexto de los documentos
        contexto = "\n\n".join([f"Documento {i+1}:\n{doc.page_content}" 
                                for i, doc in enumerate(docs_relevantes)])
        
        # PASO 3: Construir el historial conversacional para darle contexto al modelo
        # Esto permite que el modelo recuerde las preguntas anteriores
        historial_texto = ""
        if chat_history:
            historial_texto = "\n\nHistorial de conversaci√≥n anterior:\n"
            for q, a in chat_history[-3:]:  # √öltimas 3 conversaciones para no contaminar
                historial_texto += f"P: {q}\nR: {a}\n"
        
        # PASO 4: Crear un prompt m√°s simple y directo
        # Menos contexto = mejor generaci√≥n, el modelo no se confunde
        prompt = f"""Bas√°ndote en la siguiente informaci√≥n de libros de Psicolog√≠a, responde la pregunta de forma concisa:

INFORMACI√ìN:
{contexto}

PREGUNTA: {pregunta}
RESPUESTA CONCISA:"""
        
        print("‚è≥ Generando respuesta...")
        
        # PASO 5: Generar la respuesta usando el LLM
        respuesta_completa = llm.invoke(prompt)
        
        # Extraer solo la respuesta (quitar el prompt que devuelve el modelo)
        if "RESPUESTA CONCISA:" in respuesta_completa:
            respuesta = respuesta_completa.split("RESPUESTA CONCISA:")[-1].strip()
        else:
            respuesta = respuesta_completa.strip()
        
        # Si la respuesta es muy larga o vac√≠a, limpiarla
        respuesta = respuesta[:1500].strip()  # M√°ximo 1500 caracteres
        if not respuesta:
            respuesta = "No pude generar una respuesta. Intenta reformular tu pregunta."
        
        print(f"\nü§ñ Respuesta del chatbot:")
        print(respuesta)
        
        # PASO 6: Guardar en el historial para la pr√≥xima pregunta
        chat_history.append((pregunta, respuesta))
        
        # PASO 7: Mostrar los documentos de los que se extrajo la informaci√≥n
        print(f"\nüìö Documentos consultados ({len(docs_relevantes)}): ")
        for i, doc in enumerate(docs_relevantes, 1):
            fuente = doc.metadata.get('source', 'Fuente desconocida')
            pagina = doc.metadata.get('page', 'N/A')
            print(f"  {i}. {fuente} (P√°gina {pagina})")
        
        return respuesta
    
    except Exception as e:
        print(f"‚ùå Error al procesar la pregunta: {e}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# PASO 11: INTERFAZ DE USUARIO (LOOP INTERACTIVO)
# ============================================================================

def iniciar_chatbot():
    """
    Inicia el chatbot en modo conversacional
    El usuario puede hacer preguntas hasta escribir 'salir'
    El chatbot recuerda el contexto de la conversaci√≥n
    """
    
    # Verificar si el chatbot est√° completamente configurado
    if qa_chain is None:
        print("\n‚ùå ERROR: El chatbot no est√° completamente configurado")
        print("No fue posible cargar el modelo de lenguaje local.")
        print("\nPara arreglarlo:")
        print("1. Aseg√∫rate de tener PyTorch: pip install torch")
        print("2. Verifica que tienes al menos 16GB de RAM disponibles")
        print("3. Vuelve a ejecutar este archivo")
        print("\nMientras tanto, puedes usar: python test.py")
        print("(que busca documentos sin usar el modelo LLM)")
        return
    
    print("\n" + "="*60)
    print("üéì BIENVENIDO AL CHATBOT DE PSICOLOG√çA INTELIGENTE")
    print("="*60)
    print("\n‚ú® Este chatbot RECUERDA nuestra conversaci√≥n")
    print("Escribe tus preguntas sobre psicolog√≠a.")
    print("El chatbot usar√° IA para generar respuestas inteligentes.")
    print("\nEscribe 'salir' o 'quit' para terminar.")
    print("Escribe 'limpiar' para olvidar el historial.\n")
    
    while True:
        # Pedir pregunta al usuario
        pregunta = input("\nüìù Escribe tu pregunta: ").strip()
        
        # Verificar si el usuario quiere salir
        if pregunta.lower() in ["salir", "quit", "exit"]:
            print("\nüëã ¬°Hasta luego! Gracias por usar el chatbot.")
            break
        
        # Limpiar historial
        if pregunta.lower() == "limpiar":
            chat_history.clear()
            print("üßπ Historial de conversaci√≥n limpiado.")
            continue
        
        # Ignorar preguntas vac√≠as
        if not pregunta:
            continue
        
        # Hacer la pregunta al chatbot
        hacer_pregunta(pregunta)

# ============================================================================
# PASO 12: EJECUTAR EL CHATBOT
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("INICIALIZANDO CHATBOT DE PSICOLOG√çA")
    print("="*60)
    
    # Iniciar el loop interactivo
    iniciar_chatbot()
