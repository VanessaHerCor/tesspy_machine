# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘           CHATBOT INTELIGENTE CON LANGCHAIN - CARGA DE PDFs                â•‘
# â•‘                                                                              â•‘
# â•‘  Este cÃ³digo carga tus PDFs de psicologÃ­a y prepara la informaciÃ³n para     â•‘
# â•‘  entrenar un chatbot inteligente que pueda responder preguntas basadas      â•‘
# â•‘  en el contenido de tus documentos.                                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ============================================================================
# PASO 1: IMPORTAR LIBRERÃAS NECESARIAS
# ============================================================================

import os                                      # Para manejo de rutas de archivos
import glob                                    # Para buscar archivos (*. pdf)
from pathlib import Path                       # Para manejo profesional de rutas

# Importar el cargador de PDFs de LangChain
from langchain_community.document_loaders import PyPDFLoader

# â­ IMPORTAR PARA GUARDAR/CARGAR FAISS (BASE DE DATOS)
from langchain_community.vectorstores import FAISS

# ============================================================================
# PASO 2: CONFIGURAR LA RUTA DE TUS PDFs
# ============================================================================

# Define la carpeta donde estÃ¡n tus PDFs
# En tu caso es: Homework/Chatbot/PDF_PSY
pdf_folder_path = r'PDF_PSY'  # La 'r' significa "raw string" (ruta sin procesar)

# Alternativa mÃ¡s profesional con Path:
pdf_folder_path = Path('PDF_PSY')  # Esto funciona en Windows, Mac y Linux

# â­ RUTA DONDE GUARDAREMOS LA BASE DE DATOS FAISS
# Si la carpeta existe, no la recrea. Si no existe, la crea automÃ¡ticamente
faiss_db_path = Path('FAISS_DB')  # Se guardarÃ¡ en una carpeta llamada FAISS_DB

# ============================================================================
# PASO 3: VERIFICAR QUE LA CARPETA EXISTE
# ============================================================================

# Verificar si la carpeta de PDFs existe
if not os.path.exists(pdf_folder_path):
    # Si NO existe, crear la carpeta
    os.makedirs(pdf_folder_path)
    print(f'âŒ La carpeta {pdf_folder_path} no existe.')
    print(f'âœ… Se creÃ³ automÃ¡ticamente. Coloca tus PDFs ahÃ­.')
else:
    # Si existe, mostrar confirmaciÃ³n
    print(f'âœ… Carpeta encontrada: {pdf_folder_path}')

# ============================================================================
# PASO 4: BUSCAR TODOS LOS ARCHIVOS .PDF EN LA CARPETA
# ============================================================================

# glob.glob() busca todos los archivos que coincidan con el patrÃ³n
# En este caso: cualquier archivo .pdf en la carpeta PDF_PSY
pdf_files = glob.glob(f"{pdf_folder_path}/*.pdf")

# Mostrar cuÃ¡ntos PDFs encontrÃ³
print(f"\nğŸ“š PDFs encontrados: {len(pdf_files)}")
for i, pdf in enumerate(pdf_files, 1):
    print(f"   {i}. {os.path.basename(pdf)}")  # Mostrar solo el nombre del archivo

# ============================================================================
# PASO 5: CARGAR TODOS LOS PDFs Y EXTRAER SU CONTENIDO
# ============================================================================

# Esta lista almacenarÃ¡ TODAS las pÃ¡ginas de TODOS los PDFs
all_pages = []

# Recorrer cada archivo PDF encontrado
for pdf_file in pdf_files:
    print(f"\nğŸ“– Procesando: {os.path.basename(pdf_file)}...")
    
    try:
        # PASO 5a: Crear un cargador para este PDF especÃ­fico
        loader = PyPDFLoader(pdf_file)
        
        # PASO 5b: Cargar todas las pÃ¡ginas del PDF
        # Cada pÃ¡gina contiene: contenido de texto + metadatos (nombre, nÃºmero de pÃ¡gina)
        pages = loader.load()
        
        # PASO 5c: Agregar todas las pÃ¡ginas a nuestra lista general
        all_pages.extend(pages)
        
        # Mostrar cuÃ¡ntas pÃ¡ginas se extrajeron de este PDF
        print(f"   âœ… {len(pages)} pÃ¡ginas cargadas exitosamente")
        
    except Exception as e:
        # Si hay error, mostrarlo pero continuar con el siguiente PDF
        print(f"   âŒ Error al cargar: {e}")

# ============================================================================
# PASO 6: RESUMEN FINAL
# ============================================================================

print(f"\n" + "="*70)
print(f"âœ… PROCESO COMPLETADO")
print(f"="*70)
print(f"Total de pÃ¡ginas cargadas: {len(all_pages)}")
print(f"\nAhora tienes {len(all_pages)} pÃ¡ginas de contenido listas para:")
print(f"  1. Dividir en chunks (pÃ¡rrafos pequeÃ±os)")
print(f"  2. Crear embeddings (vectores)")
print(f"  3. Entrenar el chatbot")
print(f"="*70)

# ============================================================================
# PASO 7: DIVIDIR EL CONTENIDO EN CHUNKS (PÃRRAFOS PEQUEÃ‘OS)
# ============================================================================
# 
# Â¿POR QUÃ‰ dividir?
#   - Los modelos de IA no pueden procesar texto muy largo de una sola vez
#   - Es mejor tener pÃ¡rrafos pequeÃ±os y manejables
#   - Facilita buscar informaciÃ³n relevante mÃ¡s rÃ¡pido
#
# Â¿QUÃ‰ es un chunk?
#   - Un trozo de texto de ~1000 caracteres (aproximadamente 200 palabras)
#   - Los chunks se pueden traslapar (overlap) para no perder contexto
#

from langchain_text_splitters import RecursiveCharacterTextSplitter

# Crear un divisor de texto con parÃ¡metros especÃ­ficos
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Cada chunk tendrÃ¡ mÃ¡ximo 1000 caracteres
    chunk_overlap=200,      # Los chunks se superponen 200 caracteres (para contexto)
    separators=["\n\n", "\n", ".", " "]  # Separadores por orden de preferencia
)

# Dividir TODOS los documentos en chunks
chunks = text_splitter.split_documents(all_pages)

print(f"\n" + "="*70)
print(f"ğŸ“¦ CHUNKS CREADOS")
print(f"="*70)
print(f"Total de chunks: {len(chunks)}")
print(f"\nEjemplo del primer chunk:")
print(f"-" * 70)
# ============================================================================
# PASO 8: CREAR EMBEDDINGS O CARGAR LA BASE DE DATOS GUARDADA
# ============================================================================
#
# â­ OPTIMIZACIÃ“N: AquÃ­ es donde ocurre la "magia"
#    - PRIMERA VEZ: Crea embeddings (tarda ~5 minutos)
#    - SIGUIENTES VECES: Carga la base de datos guardada (tarda <1 segundo)
#

from langchain_community.embeddings import HuggingFaceEmbeddings

# Crear el modelo de embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-MiniLM-L6-v2"
)

# â­ VERIFICAR SI LA BASE DE DATOS YA EXISTE
if faiss_db_path.exists():
    # SI EXISTE: Cargar la base de datos guardada (Â¡RÃPIDO!)
    print(f"\n" + "="*70)
    print(f"âš¡ CARGANDO BASE DE DATOS GUARDADA (RÃPIDO)")
    print(f"="*70)
    print(f"ğŸ“‚ Encontrada base de datos en: {faiss_db_path}")
    print(f"â±ï¸ Cargando... (esto tarda <1 segundo)")
    
    # Cargar FAISS desde disco
    vector_store = FAISS.load_local(
        folder_path=str(faiss_db_path),
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )
    
    print(f"âœ… Base de datos cargada exitosamente")
    print(f"   ğŸ“Š Documentos en la BD: {len(chunks)} chunks")
    
else:
    # SI NO EXISTE: Crear la base de datos (proceso completo)
    print(f"\n" + "="*70)
    print(f"ğŸ§  MODELO DE EMBEDDINGS CARGADO")
    print(f"="*70)
    print(f"Modelo: sentence-transformers/paraphrase-MiniLM-L6-v2")
    print(f"Tipo de vector: 384 dimensiones (nÃºmeros por vector)")

    # Crear un embedding de prueba para mostrar cÃ³mo funciona
    print(f"\nğŸ“ Creando embedding de prueba...")
    test_text = "La psicologÃ­a es el estudio del comportamiento humano"
    test_embedding = embeddings.embed_query(test_text)
    print(f"âœ… Embedding creado: {len(test_embedding)} nÃºmeros")
    print(f"   Primeros 5 nÃºmeros: {test_embedding[:5]}")


# ============================================================================
# PASO 9: CREAR BASE DE DATOS VECTORIAL (FAISS) O REUTILIZAR LA EXISTENTE
# ============================================================================
#
# Â¿QUÃ‰ es FAISS?
#   - Base de datos especializada en almacenar vectores
#   - Permite bÃºsquedas rÃ¡pidas por SIMILITUD
#   - Usada por Google, Meta, OpenAI
#
# â­ OPTIMIZACIÃ“N: Si la base de datos existe, solo la cargamos
#

# Si NO tenÃ­amos la base de datos guardada, crearla ahora
if not faiss_db_path.exists():
    print(f"\n" + "="*70)
    print(f"ğŸ’¾ CREANDO BASE DE DATOS VECTORIAL (FAISS)")
    print(f"="*70)
    print(f"â±ï¸ Esto puede tardar 1-2 minutos (solo la primera vez)...")
    print(f"   (Las siguientes veces serÃ¡ instantÃ¡neo)")

    # Crear la base de datos vectorial a partir de los chunks
    # Cada chunk se convierte en un vector y se almacena en FAISS
    vector_store = FAISS.from_documents(
        documents=chunks,           # Los chunks a procesar
        embedding=embeddings        # El modelo de embeddings a usar
    )

    print(f"âœ… Base de datos vectorial creada")
    print(f"   Documentos indexados: {len(chunks)}")
    
    # â­ GUARDAR LA BASE DE DATOS EN DISCO
    print(f"\nğŸ’¾ Guardando base de datos para prÃ³ximas ejecuciones...")
    vector_store.save_local(folder_path=str(faiss_db_path))
    print(f"âœ… Guardado en: {faiss_db_path}")
    print(f"   PrÃ³ximas ejecuciones serÃ¡n mucho mÃ¡s rÃ¡pidas âš¡")

# ============================================================================
# PASO 10: BUSCAR INFORMACIÃ“N SIMILAR
# ============================================================================
#
# Ahora que tenemos todo preparado, podemos hacer bÃºsquedas inteligentes
#

print(f"\n" + "="*70)
print(f"ğŸ” BÃšSQUEDA INTERACTIVA EN PDFs")
print(f"="*70)
print(f"\nÂ¡Tu chatbot estÃ¡ listo para responder preguntas!")
print(f"Escribe 'salir' para terminar\n")

# Loop interactivo para hacer bÃºsquedas
while True:
    # Solicitar pregunta al usuario
    query = input("â“ Â¿QuÃ© quieres preguntar?: ").strip()
    
    # Si dice salir, terminar
    if query.lower() in ['salir', 'exit', 'quit']:
        print("\nğŸ‘‹ Â¡Hasta luego!")
        break
    
    # Si estÃ¡ vacÃ­o, pedir que escriba algo
    if not query:
        print("âš ï¸ Por favor, escribe una pregunta\n")
        continue
    
    # Buscar documentos similares
    print(f"\nğŸ” Buscando informaciÃ³n sobre: '{query}'")
    print("   (esto tarda un segundo...)\n")
    
    try:
        results = vector_store.similarity_search(query, k=3)
        
        print(f"âœ… Encontrados {len(results)} resultados similares:\n")
        
        for i, result in enumerate(results, 1):
            page_num = result.metadata.get('page', 'N/A')
            content = result.page_content[:200]
            
            print(f"Resultado {i}: (PÃ¡gina {page_num})")
            print(f"  {content}...")
            print()
        
    except Exception as e:
        print(f"âŒ Error en la bÃºsqueda: {e}\n")

# ============================================================================
# PASO 11: FIN DEL CHATBOT INTERACTIVO
# ============================================================================

print(f"\n" + "="*70)
print(f"ğŸ“‹ RESUMEN DEL PROGRESO")
print(f"="*70)
print(f"âœ… Paso 1:  PDFs cargados ({len(all_pages)} pÃ¡ginas)")
print(f"âœ… Paso 2:  Chunks creados ({len(chunks)} chunks)")
print(f"âœ… Paso 3:  Embeddings generados")
print(f"âœ… Paso 4:  Base de datos vectorial (FAISS) lista")
print(f"âœ… Paso 5:  Chat interactivo completado âœ“")
print(f"\nğŸš€ PRÃ“XIMAS MEJORAS:")
print(f"  1. Conectar con OpenAI GPT (para respuestas inteligentes)")
print(f"  2. Crear interfaz Streamlit (chat web bonito)")
print(f"  3. Agregar historial de conversaciones")
print(f"="*70)