# ============================================================================
# CHATBOT DE PSICOLOG√çA - PROYECTO FINAL
# Basado en LangChain y Sistema RAG (Retrieval-Augmented Generation)
# ============================================================================

# PASO 1: IMPORTAR LIBRER√çAS NECESARIAS
# ============================================================================

import os
import glob  # Para buscar archivos PDF
from pathlib import Path

# Librer√≠as de LangChain - el framework principal para construir chatbots
from langchain_community.document_loaders import PyPDFLoader  # Carga archivos PDF
from langchain_text_splitters import RecursiveCharacterTextSplitter  # Divide textos en fragmentos
from langchain_community.vectorstores import FAISS  # Base de datos de vectores (embeddings)
from langchain_community.embeddings import HuggingFaceEmbeddings  # Convierte texto a vectores
from langchain.chains import ConversationalRetrievalChain  # Cadena conversacional (como el profesor)
from langchain.prompts import PromptTemplate  # Template para dar instrucciones al modelo

# Para usar modelos locales (como el profesor)
from langchain_community.llms import HuggingFacePipeline  # Pipeline local de HuggingFace
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline  # Modelos locales
import torch  # Para GPU (si est√° disponible)

# ============================================================================
# PASO 2: CONFIGURAR LA RUTA A LOS PDFs
# ============================================================================

# Indicar d√≥nde est√°n guardados los archivos PDF
PDF_FOLDER = "PDF_PSY"  # Carpeta con los PDFs

# Validar que la carpeta existe
if not os.path.exists(PDF_FOLDER):
    print(f"‚ùå ERROR: La carpeta '{PDF_FOLDER}' no existe.")
    print("Por favor, crea una carpeta 'PDF_PSY' en la misma ubicaci√≥n que este archivo.")
    exit()

# Buscar todos los archivos PDF en la carpeta
pdf_files = glob.glob(os.path.join(PDF_FOLDER, "*.pdf"))

if not pdf_files:
    print(f"‚ùå ERROR: No hay archivos PDF en la carpeta '{PDF_FOLDER}'")
    exit()

print(f"‚úÖ Se encontraron {len(pdf_files)} archivos PDF")
print("Archivos cargados:")
for pdf in pdf_files:
    print(f"  - {os.path.basename(pdf)}")

# ============================================================================
# PASO 3: CARGAR Y PROCESAR LOS PDFs
# ============================================================================
print("\nüìÑ Cargando documentos PDF...")

# Lista para almacenar todos los documentos
all_documents = []

# Cargar cada PDF
for pdf_file in pdf_files:
    try:
        loader = PyPDFLoader(pdf_file)
        documents = loader.load()
        all_documents.extend(documents)
        print(f"‚úÖ Cargado: {os.path.basename(pdf_file)} ({len(documents)} p√°ginas)")
    except Exception as e:
        print(f"‚ö†Ô∏è Error al cargar {pdf_file}: {e}")

print(f"\n‚úÖ Total de documentos cargados: {len(all_documents)}")

# ============================================================================
# PASO 4: DIVIDIR DOCUMENTOS EN FRAGMENTOS PEQUE√ëOS (CHUNKS)
# ============================================================================
print("\n‚úÇÔ∏è Dividiendo documentos en fragmentos...")

# Dividir el texto en fragmentos de 1000 caracteres con 200 caracteres de solapamiento
# Esto es importante para que el modelo entienda mejor el contexto
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,      # Tama√±o de cada fragmento
    chunk_overlap=200,    # Solapamiento para no perder contexto
)

# Aplicar la divisi√≥n a todos los documentos
chunks = text_splitter.split_documents(all_documents)
print(f"‚úÖ Documentos divididos en {len(chunks)} fragmentos")

# ============================================================================
# PASO 5: CREAR EMBEDDINGS (VECTORES) DE LOS FRAGMENTOS
# ============================================================================
print("\nüß† Creando embeddings (vectores)...")

# Usar HuggingFace para crear embeddings - estos son GRATUITOS
# Los embeddings convierten texto en n√∫meros que representan el significado
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    # Este modelo funciona bien con textos en espa√±ol
)

# Nombre del archivo donde se guardar√°n los embeddings
EMBEDDINGS_PATH = "embeddings_psy"

# Verificar si ya existen embeddings guardados (para no procesarlos de nuevo)
if os.path.exists(EMBEDDINGS_PATH):
    print(f"‚úÖ Encontrados embeddings guardados en '{EMBEDDINGS_PATH}'")
    print("Cargando embeddings... (esto es MUCHO m√°s r√°pido)")
    vector_store = FAISS.load_local(EMBEDDINGS_PATH, embeddings, allow_dangerous_deserialization=True)
    print("‚úÖ Embeddings cargados exitosamente")
else:
    # Crear la base de datos de vectores usando FAISS (muy r√°pido y eficiente)
    # Esto almacena todos los embeddings y permite b√∫squedas r√°pidas
    print("Primera vez: creando embeddings...")
    print("Esto puede tardar unos momentos...")
    vector_store = FAISS.from_documents(chunks, embeddings)
    
    # GUARDAR los embeddings para la pr√≥xima vez (igual que tu profesor)
    print(f"\nüíæ Guardando embeddings en '{EMBEDDINGS_PATH}' para uso futuro...")
    vector_store.save_local(EMBEDDINGS_PATH)
    print("‚úÖ Base de datos de vectores creada y guardada exitosamente")
    print("‚ö° La pr√≥xima vez cargar√° MUCHO m√°s r√°pido")

# ============================================================================
# PASO 6: CREAR EL RETRIEVER
# ============================================================================
print("\nüîç Configurando el retriever...")

# El retriever busca los fragmentos m√°s relevantes para cada pregunta
# search_kwargs={'k': 4} significa que traer√° los 4 fragmentos m√°s similares
retriever = vector_store.as_retriever(search_kwargs={"k": 4})

print("‚úÖ Retriever configurado (buscar√° los 4 documentos m√°s relevantes)")

# ============================================================================
# PASO 7: CONFIGURAR EL MODELO DE LENGUAJE (LLM)
# ============================================================================
print("\nü§ñ Configurando el modelo de lenguaje...")

# Vamos a usar un modelo GRATUITO de HuggingFace
# Puedes cambiar el modelo seg√∫n tus necesidades
# Modelos recomendados: "mistralai/Mistral-7B-Instruct-v0.2" o "meta-llama/Llama-2-7b-chat"

# ============================================================================
# PASO 7: CONFIGURAR EL MODELO DE LENGUAJE LOCAL (LLM)
# ============================================================================
print("\nü§ñ Configurando el modelo de lenguaje local...")
print("‚ö†Ô∏è Primera vez: descargar√° ~7GB (puede tardar 10-20 minutos)...")

llm = None  # Inicializar

try:
    # Usar Microsoft Phi-2: m√°s peque√±o que Mistral pero muy poderoso
    # Es lo que recomend√≥ tu profesor
    model_id = "microsoft/phi-2"
    
    print(f"üì• Descargando modelo: {model_id}")
    print("Este proceso solo ocurre la primera vez...")
    
    # Cargar el tokenizador
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    # Cargar el modelo (esto descargar√° ~7GB)
    # torch_dtype=torch.float16 lo hace m√°s peque√±o y r√°pido
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        trust_remote_code=True,
        torch_dtype=torch.float16  # Usar 16-bit para usar menos memoria
    )
    
    # Crear el pipeline de generaci√≥n de texto
    # max_new_tokens controla cu√°n larga ser√° la respuesta
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512  # Respuestas moderadas
    )
    
    # Envolver en HuggingFacePipeline para LangChain
    llm = HuggingFacePipeline(pipeline=pipe)
    
    print("‚úÖ Modelo de lenguaje configurado correctamente")
    print(f"‚úÖ Usando: {model_id}")
    
except Exception as e:
    print(f"‚ùå Error al cargar el modelo: {e}")
    print("Aseg√∫rate de tener:")
    print("  - 16GB+ RAM disponible")
    print("  - PyTorch instalado: pip install torch")
    print("  - Conexi√≥n a internet (para descargar el modelo)")
    llm = None

# ============================================================================
# PASO 8: CREAR EL TEMPLATE DE PREGUNTA (PROMPT)
# ============================================================================

# Este template define c√≥mo se le formula la pregunta al modelo
# Incluye el contexto (documentos relevantes) y la pregunta del usuario
prompt_template = """Eres un asistente experto en Psicolog√≠a. 
Usa la siguiente informaci√≥n para responder la pregunta de manera clara y completa.
Si no sabes la respuesta, di que no tienes la informaci√≥n disponible.

CONTEXTO:
{context}

PREGUNTA:
{question}

RESPUESTA:"""

# Crear el prompt usando el template
PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# ============================================================================
# PASO 9: CREAR LA CADENA CONVERSACIONAL (COMO EL PROFESOR)
# ============================================================================
print("\n‚õìÔ∏è Creando la cadena conversacional...")

qa_chain = None

if llm is not None:
    # Usar ConversationalRetrievalChain (igual que el profesor)
    # Esta cadena:
    # 1. Recuerda el historial de conversaci√≥n
    # 2. Busca documentos relevantes
    # 3. Genera respuestas basadas en los documentos
    
    try:
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,                              # Modelo de lenguaje local
            retriever=retriever,                  # El retriever que configuramos
            return_source_documents=True         # Mostrar de d√≥nde sac√≥ la informaci√≥n
        )
        print("‚úÖ Cadena conversacional lista para usar")
    except Exception as e:
        print(f"‚ö†Ô∏è Error al crear la cadena: {e}")
        qa_chain = None
else:
    print("‚ö†Ô∏è No se puede crear la cadena sin el modelo LLM")
    print("Verifica que el modelo se carg√≥ correctamente")

# ============================================================================
# PASO 10: VARIABLE PARA GUARDAR EL HISTORIAL DE CHAT
# ============================================================================

# El historial se guarda aqu√≠ para que la IA recuerde la conversaci√≥n
chat_history = []

# ============================================================================
# PASO 11: FUNCI√ìN PARA HACER PREGUNTAS AL CHATBOT
# ============================================================================

def hacer_pregunta(pregunta):
    """
    Funci√≥n para hacer una pregunta al chatbot con historial conversacional
    
    Args:
        pregunta (str): La pregunta que deseas hacer
    """
    global chat_history  # Usar el historial global
    
    print(f"\nüë§ Tu pregunta: {pregunta}")
    print("-" * 60)
    
    if qa_chain is None:
        print("‚ùå El chatbot no est√° disponible")
        return
    
    try:
        # Hacer la pregunta a la cadena
        # El historial le permite al modelo recordar conversaciones previas
        result = qa_chain.invoke({
            "question": pregunta,
            "chat_history": chat_history
        })
        
        # Extraer la respuesta
        respuesta = result.get("answer", "No se pudo obtener respuesta")
        
        print(f"\nü§ñ Respuesta del chatbot:")
        print(respuesta)
        
        # Guardar en el historial para la pr√≥xima pregunta
        # Esto le permite al chatbot recordar
        chat_history.append((pregunta, respuesta))
        
        # Mostrar los documentos de los que se extrajo la informaci√≥n
        if "source_documents" in result:
            print(f"\nüìö Documentos consultados ({len(result['source_documents'])}): ")
            for i, doc in enumerate(result['source_documents'], 1):
                fuente = doc.metadata.get('source', 'Fuente desconocida')
                pagina = doc.metadata.get('page', 'N/A')
                print(f"  {i}. {fuente} (P√°gina {pagina})")
        
        return result
    
    except Exception as e:
        print(f"‚ùå Error al procesar la pregunta: {e}")
        return None

# ============================================================================
# PASO 11: INTERFAZ DE USUARIO (LOOP INTERACTIVO)
# ============================================================================

def iniciar_chatbot():
    """
    Inicia el chatbot en modo conversacional
    El usuario puede hacer preguntas hasta escribir 'salir'
    El chatbot recuerda el contexto de la conversaci√≥n
    """
    
    # Verificar si el chatbot est√° completamente configurado
    if qa_chain is None:
        print("\n‚ùå ERROR: El chatbot no est√° completamente configurado")
        print("No fue posible cargar el modelo de lenguaje local.")
        print("\nPara arreglarlo:")
        print("1. Aseg√∫rate de tener PyTorch: pip install torch")
        print("2. Verifica que tienes al menos 16GB de RAM disponibles")
        print("3. Vuelve a ejecutar este archivo")
        print("\nMientras tanto, puedes usar: python test.py")
        print("(que busca documentos sin usar el modelo LLM)")
        return
    
    print("\n" + "="*60)
    print("üéì BIENVENIDO AL CHATBOT DE PSICOLOG√çA INTELIGENTE")
    print("="*60)
    print("\n‚ú® Este chatbot RECUERDA nuestra conversaci√≥n")
    print("Escribe tus preguntas sobre psicolog√≠a.")
    print("El chatbot usar√° IA para generar respuestas inteligentes.")
    print("\nEscribe 'salir' o 'quit' para terminar.")
    print("Escribe 'limpiar' para olvidar el historial.\n")
    
    while True:
        # Pedir pregunta al usuario
        pregunta = input("\nüìù Escribe tu pregunta: ").strip()
        
        # Verificar si el usuario quiere salir
        if pregunta.lower() in ["salir", "quit", "exit"]:
            print("\nüëã ¬°Hasta luego! Gracias por usar el chatbot.")
            break
        
        # Limpiar historial
        if pregunta.lower() == "limpiar":
            chat_history.clear()
            print("üßπ Historial de conversaci√≥n limpiado.")
            continue
        
        # Ignorar preguntas vac√≠as
        if not pregunta:
            continue
        
        # Hacer la pregunta al chatbot
        hacer_pregunta(pregunta)

# ============================================================================
# PASO 12: EJECUTAR EL CHATBOT
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("INICIALIZANDO CHATBOT DE PSICOLOG√çA")
    print("="*60)
    
    # Iniciar el loop interactivo
    iniciar_chatbot()
