COLAB 1
## Introducción a Bag of Words (La Bolsa de Palabras)

**Concepto:** Bag of Words (BoW) es una técnica de representación vectorial de texto muy simple y fundamental en el Procesamiento del Lenguaje Natural (PLN). Su nombre, "Bolsa de Palabras", describe perfectamente su funcionamiento: trata un documento de texto como una colección (o "bolsa") de sus palabras, ignorando por completo el orden en que aparecen, la gramática y el contexto en el que se usan. Solo se enfoca en la frecuencia de cada palabra dentro del documento.

**Analogía:** Imagina que tienes varias oraciones y una bolsa vacía. Tomas cada palabra de cada oración, la metes en la bolsa y al final cuentas cuántas tienes de cada una. Lo que queda es una representación de cada oración basada únicamente en la cuenta de sus palabras, sin importar cómo estaban originalmente ordenadas.

**Pasos Generales:**

1.  **Recopilación de Documentos:** Se parte de una colección de documentos de texto (corpus).
2.  **Creación del Vocabulario:** Se identifican todas las palabras únicas que aparecen en todos los documentos del corpus. Este conjunto de palabras únicas forma el vocabulario.
3.  **Vectorización:** Para cada documento, se crea un vector numérico. Cada dimensión de este vector corresponde a una palabra en el vocabulario, y el valor en esa dimensión representa la frecuencia con la que esa palabra aparece en el documento. Si una palabra del vocabulario no aparece en un documento, su valor en el vector será cero.

En esencia, BoW transforma el texto en datos numéricos que pueden ser utilizados por algoritmos de aprendizaje automático.

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'el perro come la comida',
    'el gato come pescado, el gato orina, el gato mira a la ventana',
    'el perro juega con el gato'
]

# 1. Creamos el vectorizador
vectorizer = CountVectorizer()

# 2. Creamos la matriz de Bag of Words
X = vectorizer.fit_transform(corpus)

# 3. Creamos un DataFrame de pandas para visualizar la matriz
bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

# Mostramos el DataFrame
display(bow_df)

**Limitación Clave:** BoW le da la misma importancia a todas las palabras. En nuestro ejemplo, la palabra "el" tiene el conteo más alto, pero es la que menos significado aporta.

## Técnica 2 - TF-IDF (El Detective de Palabras Clave)

**Concepto:** TF-IDF (Term Frequency-Inverse Document Frequency) es una versión mejorada de BoW. Su objetivo es darle más peso a las palabras que son importantes en un documento específico, pero no comunes en todos los demás.

**La Fórmula Mágica:** TF-IDF = TF * IDF

*   **TF (Frecuencia del Término):** ¿Qué tan frecuente es una palabra en un solo documento? Es básicamente lo mismo que BoW. Una palabra que aparece mucho en un documento es probablemente importante para ese documento.
*   **IDF (Frecuencia Inversa del Documento):** ¿Qué tan rara o común es una palabra en toda la colección de documentos? Esta es la parte clave.
    *   Palabras muy comunes en todos los documentos (como "el", "es", "y") obtienen una puntuación IDF baja.
    *   Palabras raras que solo aparecen en unos pocos documentos obtienen una puntuación IDF alta.

**Intuición:** El puntaje TF-IDF será alto cuando una palabra aparece muchas veces en un documento (TF alto) pero es rara en el resto de la colección (IDF alto). Esto resalta las palabras que definen un texto.

En nuestro ejemplo de BoW, "pescado" y "juega" tendrían un TF-IDF alto porque aparecen solo en un documento cada una. "el" y "come" tendrían un TF-IDF bajo.

**ATENCIÓN**: La siguiente formula se presenta como lo qué "hace" por detrás la formula. Sin embargo, si se realiza, no saldrá el resultado como sale con el código. La razón es que scikit-learn (la librería que estamos usando) aplica una versión más avanzada y estandarizada de las fórmulas para hacerlas más robustas en aplicaciones reales.

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd # Import pandas for better visualization later

corpus = [
    'el perro come carne',
    'el gato come pescado, el perro comida',
    'el perro juega con el gato'
]

# 1. Creamos el vectorizador TF-IDF
tfidf_vectorizer = TfidfVectorizer()

# 2. Creamos la matriz
X_tfidf = tfidf_vectorizer.fit_transform(corpus)

# 3. Creamos un DataFrame para visualizar la matriz (opcional, pero útil)
tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# 4. Mostramos el DataFrame
print("Vocabulario:", tfidf_vectorizer.get_feature_names_out())
print("\nMatriz TF-IDF:\n")
display(tfidf_df)

# **Resumen Final:**

Bag of Words: Rápido y simple. Cuenta la frecuencia de las palabras. Su debilidad es que trata a todas las palabras por igual.

TF-IDF: Más inteligente. Encuentra las palabras que son distintivas de un documento, dándoles más importancia. Es el estándar para muchas tareas de NLP.

COLAB 2

!pip install gensim transformers torch

from gensim.models import Word2Vec, FastText
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

corpus_w2v = [
    ['rey', 'es', 'un', 'hombre', 'poderoso'],
    ['reina', 'es', 'una', 'mujer', 'poderosa'],
    ['mujer', 'es', 'inteligente'],
    ['el', 'rey', 'gobierna', 'el', 'reino'],
    ['la', 'reina', 'gobierna', 'el', 'reino']
]

# **Sección 1: Word2Vec - Embeddings Estáticos**

# Entrenamos un modelo Word2Vec simple
# vector_size: la dimensionalidad de nuestros vectores (embeddings)
# window: el tamaño de la ventana de contexto
# min_count: ignorar palabras con una frecuencia menor a 1
model_w2v = Word2Vec(corpus_w2v, vector_size=10, window=5, min_count=1, workers=4)

# Obtenemos el vector (embedding) para una palabra
vector_rey = model_w2v.wv['reina']
print(f"Vector para 'rey' (10 dimensiones):\n{vector_rey}\n")

# Encontramos las palabras más similares
print("Palabras más similares a 'rey':")
print(model_w2v.wv.most_similar('rey'), "\n")

print("Palabras más similares a 'mujer':")
print(model_w2v.wv.most_similar('mujer'), "\n")

# Un rey siempre necesita de una reina
# Realizamos la famosa analogía: rey - hombre + mujer ≈ reina
print("Calculando: rey - hombre + mujer...")
resultado_analogia = model_w2v.wv.most_similar(positive=['rey', 'mujer'], negative=['hombre'], topn=3)
print(f"El resultado más probable es: {resultado_analogia}\n")

# **Sección 2: FastText - Manejando Palabras Desconocidas**

# Entrenamos un modelo FastText con el mismo corpus
model_ft = FastText(corpus_w2v, vector_size=10, window=5, min_count=1, workers=4)

# FastText puede generar un vector para una palabra que NO está en el vocabulario
# porque usa sub-palabras (n-gramas de caracteres).
try:
    # Esto daría un error con Word2Vec
    vector_realeza = model_ft.wv['realeza']
    print("FastText SÍ puede crear un vector para una palabra desconocida como 'realeza':")
    print(vector_realeza, "\n")
except KeyError as e:
    print(e)

# También funciona con errores tipográficos
vector_reina_typo = model_ft.wv['reinaa']
print("FastText también puede manejar errores tipográficos como 'reinaa':")
print(vector_reina_typo, "\n")

# **Sección 3: BERT - La Revolución del Contexto**

# -*- coding: utf-8 -*-
"""
Este script demuestra el poder contextual de BERT usando una palabra
con múltiples significados no relacionados: "planta".
"""

# Importaciones necesarias
from transformers import BertTokenizer, BertModel
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# --- Configuración del Modelo ---
print("Cargando modelo pre-entrenado de BERT en español (puede tardar un momento)...")
tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')
model_bert = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-cased')

# --- Oraciones con la palabra "planta" en contextos más ricos y largos ---
oracion_ser_vivo = "La planta de tomates que sembramos en el jardín necesita mucho sol y agua para crecer"
oracion_fabrica = "La planta de ensamblaje de coches en la zona industrial tuvo que cerrar por falta de suministros"
oracion_cuerpo = "El atleta sintió un dolor agudo en toda la planta del pie después de correr la maratón"

def get_bert_contextual_embedding(text, target_word, model, tokenizer):
    """
    Obtiene el embedding contextual de una palabra promediando las últimas 4 capas de BERT.
    """
    inputs = tokenizer(text, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)

    hidden_states = outputs.hidden_states
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    # --- Diagnóstico: Imprimimos los tokens ---
    print(f"\nTokens para la oración: '{text}'")
    print(tokens)

    try:
        target_word_index = tokens.index(target_word)
    except ValueError:
        print(f"-> ERROR: La palabra '{target_word}' no se encontró como un token único.")
        return None

    last_four_layers = [hidden_states[i] for i in (-1, -2, -3, -4)]
    token_embeddings = torch.stack(last_four_layers).sum(0)
    word_embedding = token_embeddings[0][target_word_index]

    return word_embedding.numpy()

# --- Obtenemos los 3 vectores diferentes para "planta" ---
vector_planta_ser_vivo = get_bert_contextual_embedding(oracion_ser_vivo, 'planta', model_bert, tokenizer)
vector_planta_fabrica = get_bert_contextual_embedding(oracion_fabrica, 'planta', model_bert, tokenizer)
vector_planta_cuerpo = get_bert_contextual_embedding(oracion_cuerpo, 'planta', model_bert, tokenizer)

if all(v is not None for v in [vector_planta_ser_vivo, vector_planta_fabrica, vector_planta_cuerpo]):
    # --- Comparamos las similitudes ---
    sim_ser_vivo_fabrica = cosine_similarity([vector_planta_ser_vivo], [vector_planta_fabrica])[0][0]
    sim_ser_vivo_cuerpo = cosine_similarity([vector_planta_ser_vivo], [vector_planta_cuerpo])[0][0]
    sim_fabrica_cuerpo = cosine_similarity([vector_planta_fabrica], [vector_planta_cuerpo])[0][0]

    print("\n--- Similitud del Coseno entre los Embeddings de 'Planta' ---")
    print(f"Ser Vivo vs. Fábrica: {sim_ser_vivo_fabrica:.2f}")
    print(f"Ser Vivo vs. Cuerpo:  {sim_ser_vivo_cuerpo:.2f}")
    print(f"Fábrica vs. Cuerpo:   {sim_fabrica_cuerpo:.2f}\n")

    if max(sim_ser_vivo_fabrica, sim_ser_vivo_cuerpo, sim_fabrica_cuerpo) < 0.8:
        print("¡Éxito! Los valores de similitud son bajos, lo que demuestra que BERT")
        print("creó tres representaciones vectoriales distintas para 'planta' basadas en el contexto.")
    else:
        print("La diferencia es medible pero más sutil de lo esperado.")

